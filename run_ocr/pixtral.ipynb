{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c54cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade vllm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ada481",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade mistral_common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9e4e8a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import base64\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from google.colab import drive\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "from vllm import LLM\n",
    "from vllm.sampling_params import SamplingParams\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "try:\n",
    "    hf_token = userdata.get('hf_token')\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "        print(\"Hugging Face girişi başarılı! Token kullanılacak.\")\n",
    "    else:\n",
    "        print(\"HF_TOKEN gizli anahtarı bulunamadı. Lütfen Colab Secrets'a ekleyin.\")\n",
    "except userdata.SecretNotFoundError:\n",
    "    print(\"HF_TOKEN adlı gizli anahtar bulunamadı.\")\n",
    "except Exception as e:\n",
    "    print(f\"Hugging Face girişi sırasında bir hata oluştu: {e}\")\n",
    "\n",
    "# Google Drive connection\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return filename.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    except Exception as e:\n",
    "        print(f\"Base64 encoding failed for {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_pixtral_on_existing_json(image_root, input_json_path, model_path = \"mistralai/Pixtral-12B-2409\"):\n",
    "    \"\"\"Appends Pixtral result to existing JSON file.\"\"\"\n",
    "\n",
    "\n",
    "    print(\"Using device: VLLM managed\")\n",
    "\n",
    "    print(\"Initializing models...\")\n",
    "    model_name = model_path.split(\"/\")[-1].lower()\n",
    "    sampling_params = SamplingParams(max_tokens=8192)\n",
    "\n",
    "    # LLM object must be created only once \n",
    "    llm = LLM(model=model_path, tokenizer_mode=\"mistral\", max_model_len=8192, trust_remote_code=True)\n",
    "\n",
    "    # Load JSON files\n",
    "    with open(input_json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for filename in tqdm([k for k in data if not k.startswith(\"_\")], desc=\"OCR process\"):\n",
    "        # create road\n",
    "        for root, _, files in os.walk(image_root):\n",
    "            if 'text_category' in root:\n",
    "                continue\n",
    "            if filename in files:\n",
    "                img_path = os.path.join(root, filename)\n",
    "                try:\n",
    "                    image_base64 = encode_image_to_base64(img_path)\n",
    "                    if image_base64 is None:\n",
    "                        extracted_text = \"\"\n",
    "                    else:\n",
    "                        prompt = (\n",
    "                            \"Extract the Turkish text from the image exactly as it appears. \"\n",
    "                            \"Do not repeat, comment, translate, or add any text. \"\n",
    "                            \"Return only the raw text. If no text is detected, return an empty response.\"\n",
    "                        )\n",
    "\n",
    "                        messages = [\n",
    "                            {\n",
    "                                \"role\": \"user\",\n",
    "                                \"content\": [\n",
    "                                    {\"type\": \"text\", \"text\": prompt},\n",
    "                                    {\n",
    "                                        \"type\": \"image_url\",\n",
    "                                        \"image_url\": {\n",
    "                                            \"url\": f\"data:image/png;base64,{image_base64}\"\n",
    "                                        }\n",
    "                                    }\n",
    "                                ],\n",
    "                            }\n",
    "                        ]\n",
    "\n",
    "                        outputs = llm.chat(messages, sampling_params=sampling_params)\n",
    "                        extracted_text = outputs[0].outputs[0].text.strip()\n",
    "\n",
    "                    #write OCR results\n",
    "                    data[filename][\"models\"][model_name] = {\n",
    "                        \"prediction\": extracted_text,\n",
    "                        \"cer\": None,\n",
    "                        \"wer\": None\n",
    "                    }\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"{filename} error: {str(e)}\")\n",
    "                break  \n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    elapsed = round(time.time() - start_time,2)\n",
    "    print(f\"\\n OCR completed: {elapsed:.2f} seconds\")\n",
    "\n",
    "    meta = data.get(\"_meta\", {})\n",
    "    processing_times = meta.get(\"processing_times\", {})\n",
    "    processing_times[model_name] = elapsed\n",
    "    meta[\"processing_times\"] = processing_times\n",
    "     # Updated total image count \n",
    "\n",
    "    data[\"_meta\"] = meta\n",
    "\n",
    "    # Save updated JSON \n",
    "    with open(input_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Updated JSON saved: {input_json_path}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "input_json_path = '/content/drive/MyDrive/nutuk/benchmark/converted_data.json'\n",
    "image_root = '/content/drive/MyDrive/nutuk/benchmark/'\n",
    "\n",
    "\n",
    "# start OCR process\n",
    "updated = run_pixtral_on_existing_json(image_root, input_json_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
