{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa751fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/deepseek-ai/DeepSeek-VL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b4bad0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%cd DeepSeek-VL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78981300",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00e9941",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4\n",
    "#after changing numpy version re-start rerun "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b519d6b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01283bb8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install xformers==0.0.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e2959e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install torch==2.0.1+cu118 torchaudio==2.0.2+cu118 torchvision==0.15.2+cu118 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa1d542",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM\n",
    "from google.colab import drive\n",
    "from deepseek_vl2.models import DeepseekVLV2Processor, DeepseekVLV2ForCausalLM\n",
    "from deepseek_vl2.utils.io import load_pil_images\n",
    "\n",
    "# Google Drive connection\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return filename.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "\n",
    "def run_deepseek_on_existing_json(image_root, input_json_path, model_path=\"deepseek-ai/deepseek-vl2-tiny\"):\n",
    "    \"\"\"Appends  Deepseek VL2 result to existing JSON file.\"\"\"\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    print(\"Initializing models...\")\n",
    "    model_name = model_path.split(\"/\")[-1].lower()\n",
    "    processor = DeepseekVLV2Processor.from_pretrained(model_path)\n",
    "    vl_gpt = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model = vl_gpt.to(torch.bfloat16).to(device).eval()\n",
    "\n",
    "    #Load current JSON\n",
    "    with open(input_json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for filename in tqdm([k for k in data if not k.startswith(\"_\")], desc=\"OCR process\"):\n",
    "        for root, _, files in os.walk(image_root):\n",
    "            if 'text_category' in root:\n",
    "                continue\n",
    "            if filename in files:\n",
    "                img_path = os.path.join(root, filename)\n",
    "                try:\n",
    "                    conversation = [\n",
    "                        {\n",
    "                            \"role\": \"<|User|>\",\n",
    "                            \"content\": \"<image>\\n<|ref|>Extract the Turkish text exactly.<|/ref|>.\",\n",
    "                            \"images\": [img_path],\n",
    "                        },\n",
    "                        {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n",
    "                    ]\n",
    "\n",
    "                    pil_images = load_pil_images(conversation)\n",
    "\n",
    "                    prepare_inputs = processor(\n",
    "                        conversations=conversation,\n",
    "                        images=pil_images,\n",
    "                        force_batchify=True,\n",
    "                        system_prompt=\"\"\n",
    "                    ).to(model.device)\n",
    "\n",
    "                    inputs_embeds = model.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "                    outputs = model.generate(\n",
    "                        inputs_embeds=inputs_embeds,\n",
    "                        attention_mask=prepare_inputs.attention_mask,\n",
    "                        pad_token_id=processor.tokenizer.eos_token_id,\n",
    "                        bos_token_id=processor.tokenizer.bos_token_id,\n",
    "                        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                        max_new_tokens=512,\n",
    "                        do_sample=False,\n",
    "                        use_cache=True,\n",
    "                    )\n",
    "\n",
    "                    extracted_text = processor.tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "\n",
    "                    #write OCR results\n",
    "                    data[filename][\"models\"][model_name] = {\n",
    "                        \"prediction\": extracted_text,\n",
    "                        \"cer\": None,\n",
    "                        \"wer\": None\n",
    "                    }\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"{filename} error: {str(e)}\")\n",
    "                break  \n",
    "\n",
    "\n",
    "    elapsed = round(time.time() - start_time,2)\n",
    "    print(f\"\\n OCR completed: {elapsed:.2f} seconds\")\n",
    "\n",
    "    meta = data.get(\"_meta\", {})\n",
    "    processing_times = meta.get(\"processing_times\", {})\n",
    "    processing_times[model_name] = elapsed\n",
    "    meta[\"processing_times\"] = processing_times\n",
    "    data[\"_meta\"] = meta\n",
    "\n",
    "    # save updated JSON\n",
    "    with open(input_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\" Updated JSON saved: {input_json_path}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_json_path = '/content/drive/MyDrive/nutuk/benchmark/converted_data.json'\n",
    "image_root = '/content/drive/MyDrive/nutuk/benchmark/'\n",
    "\n",
    "\n",
    "# start OCR process\n",
    "updated = run_deepseek_on_existing_json(image_root, input_json_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
