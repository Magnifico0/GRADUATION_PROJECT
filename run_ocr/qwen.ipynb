{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f3bfd2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install qwen-vl-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ccf1fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CUDA ve Torch control\n",
    "!nvcc --version\n",
    "!python -c \"import torch; print(torch.__version__)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963de89",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc6d8cd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e801c9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers accelerate einops Pillow packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f321788b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575234d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from google.colab import drive\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from flash_attn import flash_attn_func\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import time  #for measuring time\n",
    "\n",
    "\n",
    "#Memory management optimization\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "def read(image_path, processor, model):\n",
    "    try:\n",
    "        question = \"Extract the Turkish text from the image exactly as it appears. Do not repeat, comment, translate, or add any text. Return only the raw text. If no text is detected, return an empty response.\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"image\": image_path,\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": question},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        text = processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "        inputs = processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = inputs.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        output_text = processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "\n",
    "        ocr_text = output_text[0]\n",
    "\n",
    "        del inputs, generated_ids, generated_ids_trimmed, output_text\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        return ocr_text\n",
    "    except Exception as e:\n",
    "        return f\"Error processing image: {str(e)}\"\n",
    "\n",
    "def run_qwen_on_existing_json(image_root, input_json_path):\n",
    "    \"\"\"Appends Qwen result to existing JSON file.\"\"\"\n",
    "\n",
    "\n",
    "    cuda = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {cuda}\")\n",
    "\n",
    "    print(\"Initializing models...\")\n",
    "\n",
    "    model_path = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "    model_name = model_path.split(\"/\")[-1].lower()\n",
    "\n",
    "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        model_path, torch_dtype=torch.bfloat16,attn_implementation=\"flash_attention_2\", device_map=\"auto\"\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "    # Load existing JSON\n",
    "    with open(input_json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for filename in tqdm([k for k in data if not k.startswith(\"_\")], desc=\"OCR process\"):       \n",
    "        for root, _, files in os.walk(image_root):\n",
    "            if 'text_category' in root:\n",
    "                continue\n",
    "            if filename in files:\n",
    "                img_path = os.path.join(root, filename)\n",
    "                try:\n",
    "                    extracted_text = read(img_path, processor, model)\n",
    "\n",
    "                    # write ocr result\n",
    "                    data[filename][\"models\"][model_name] = {\n",
    "                        \"prediction\": extracted_text,\n",
    "                        \"cer\": None,\n",
    "                        \"wer\": None\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(f\"{filename} error: {str(e)}\")\n",
    "                break  \n",
    "\n",
    "        #gc.collect()\n",
    "        #torch.cuda.empty_cache()\n",
    "        #torch.cuda.synchronize()\n",
    "\n",
    "    elapsed = round(time.time() - start_time,2)\n",
    "    print(f\"\\n OCR finished: {elapsed:.2f} saniye\")\n",
    "\n",
    "    meta = data.get(\"_meta\", {})\n",
    "    processing_times = meta.get(\"processing_times\", {})\n",
    "    processing_times[model_name] = elapsed\n",
    "    meta[\"processing_times\"] = processing_times\n",
    "    data[\"_meta\"] = meta\n",
    "\n",
    "    # Save updated JSON file\n",
    "    with open(input_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Updated JSON saved: {input_json_path}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "input_json_path = '/content/drive/MyDrive/nutuk/benchmark/converted_data.json'\n",
    "image_root = '/content/drive/MyDrive/nutuk/benchmark/'\n",
    "\n",
    "\n",
    "#start OCR process\n",
    "updated = run_qwen_on_existing_json(image_root, input_json_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
